{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1506675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacho/Documents/Maestria_IA/PLN2/CEIA-PLN2/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, PodSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d85f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf):\n",
    "    texto = \"\"\n",
    "    with pdfplumber.open(ruta_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            texto += pagina.extract_text() + \"\\n\"\n",
    "    return texto\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto_cv = extraer_texto_pdf(\"cv_de_pedro_es.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d58761d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGNACIO TOMÁS DE PEDRO MERMIER\\nINFORMACIÓN PERSONAL\\nNombre: Ignacio Tomás de Pedro Mermier Nacionalidad: Argentina\\nFecha de nacimiento: 14/01/1995 Edad: 29 años\\nCorreo electrónico: idepedro@fi.uba.ar\\nEXPERIENCIA LABORAL\\n2023 - Actualidad — Allegro Microsystems — Analog Design Engineer\\nDiseño de circuitos analógicos a nivel de bloque y top para circuitos\\nintegrados orientados a sensores magnéticos e inductivos. Verificación por\\nmedio de simulaciones por medio de herramientas específicas tales como\\nADE, SDE and AMS-DE. Desarrollo de scripts de python para analizar\\nresultados y realizar verificaciones a nivel de wafer.\\n2021 - 2023 — Allegro Microsystems — Layout Design Engineer\\nResponsable del diseño, floorplaning e implementación de layout de circuitos\\nde señales mixtas, dentro del sector dedicado a sensores magnéticos\\n(MPSBU). Desarrollo de script para mejorar el funcionamiento del PDK.\\n2018 - 2023 — Facultad de Ingeniería, Universidad de Buenos Aires — Ayudante de\\nprimera\\nAyudante de primera dentro de la asignatura “86.03 Dispositivos\\nSemiconductores”. Curso orientado a la explicación del funcionamiento\\nfísico, electrónico y circuito de distintos dispositivos tales como diodo PN,\\nTBJ, MOSFET y dispositivos optoelectrónicos.\\n2020 - 2021 — Mirgor — Ingeniero de sistemas embebidos\\nDesarrollo de software embebido para la fabricación de cámaras IP con el\\nobjetivo de filmar y transmitir en vivo, utilizando el protocolo RTSP hacia un\\nservidor web. Activación de transmisión ante la detección de eventos.\\nAplicaciones desarrolladas en C, C++ y Linux Embebido.\\n2019 - Actualidad — Instituto Nacional de Semillas (INASE) — Full Stack Developer\\nDesarrollo de la plataforma web del INASE, incluyendo manejo de FrontEnd,\\nBackEnd y administración de la Base de Datos, utilizando Django (Python),\\nCakePHP y Flutter SDK.\\n2019 - 2020 — Facultad de Ingeniería, Universidad de Buenos Aires— Pasante\\nLABCATyP\\nPasante dentro del laboratorio de Control, Automatización, Tracción y\\nPotencia dentro del marco del desarrollo de la tesis de grado para ingeniería\\nelectrónica.\\n2017 - 2018 — Holters Schule — Profesor Suplente\\nProfesor suplente en las asignaturas “Aplicaciones Electrónicas II y III” y\\n“Aplicaciones Industriales\"\\n2014 — Key Digital S.R.L — Soporte Técnico\\nMantenimiento y soporte de los equipos electrónicos y redes locales de las\\nestaciones de servicio de YPF.\\nEDUCACIÓN\\nFacultad de Ingeniería Universidad de Buenos Aires — 2024 - Actualidad\\nTítulo: Master en Inteligencia Artificial Embebida.\\nFacultad de Ingeniería Universidad de Buenos Aires — 2014 - 2019\\nTítulo: Ingeniero Electrónico. Especialización en microelectrónica y sistemas\\nembebidos. Promedio: 9.06\\nIMT Atlantique, Francia — 2018-2019\\nEstudiante de segundo año en IMT Atlantique, Brest, para la carrera de\\ningeniero generalista bajo la beca del programa ARFITEC\\nUniversity of Central Arkansas, Estados Unidos — 2017\\nEstudiante de intercambio año en University of Central Arkansas, dentro del\\nmarco de la beca “Friends of Fulbright\"\\nACTIVIDADES DE INVESTIGACIÓN\\nTesis de Ingeniería, Facultad de Ingeniería Universidad de Buenos Aires — Marzo\\n2019 - Diciembre 2019\\nTítulo: “Estudio de arreglos serie de transistores de potencia basados en SiC\\ny diseño de un circuito CMOS de mando en cascada con salida de GaN”\\nLugar de trabajo: Laboratorio de Microelectrónica - Laboratorio de Control\\nde Accionamientos, Tracción y Potencia\\nProyecto UBACyT — 2019 - 2020\\nTítulo: \"Nuevas estructuras y técnicas de simulación y control para\\nconvertidores estáticos y generadores de pulsos.” (código 20020170100386BA)\\nLugar de trabajo: Laboratorio de Control de Accionamientos, Tracción y\\nPotencia\\nFORMACIÓN COMPLEMENTARIA\\nEscuela Argentina de Micro y Nano Tecnología y Aplicaciones (EAMTA)\\n- Machine Learning en VLSI - Presentador: Dr. Ing. Pedro Julian\\n- Track básico VLSI - 2017 - Presentador: Dr. Ing. Pedro Julián\\n- Diseño de circuitos integrados para aplicaciones implacables - 2020 -\\nPresentador: Dr. Ing. Joel Gak\\nNanoelectronics Workshop (UTN FRBA)\\n- 2017 - Presentador: Dr. Lic. Felix Palumbo\\nAdvanced Analog Layout Course - IC Mask Design\\n- 2022 - Presentador: Ciaran White\\nPREMIOS Y DISTINCIONES\\nGanador de las Olimpiadas Internacionales de Microelectrónica — 2022 — Ereván,\\nArmenia\\nPrimer premio en las olimpiadas Internacionales de Microelectrónica, organizadas\\npor Synospys, Viva MTS y UNICOMP.\\nBeca Fulbright— 2017 — Comisión Fulbright, Embajada de Estados Unidos en Argentina.\\nBeca de grado para estudiar en \"University of Central Arkansas\", Arkansas, Estados\\nUnidos. Bajo el programa Friends of Fulbright\\nBeca ARFITEC — 2018 — Ministerio de Educación\\nBeca de grado para estudiar en \"IMT Atlantique\", Brest, Francia.\\nBeca Estímulo UBA — 2019 — Universidad de Buenos Aires\\nBeca obtenida para el desarrollo de la tesis de grado dentro de los laboratorios de\\nMicroelectrónica (uELAB) y Laboratorio de Control, Automatización, Tracción y\\nPotencia, dentro del Facultad de Ingeniería Universidad de Buenos Aires (FIUBA).\\nEstudiante destacado— 2017 y 2019 - Universidad de Buenos Aires.\\nPROYECTOS\\nDiseño de un sistema de posicionamiento en tiempo real por medio de la\\ntecnología Ultra Wide Band (UWB) utilizando el módulo DWM1000 — 2020\\nProyecto personal\\nDiseño de un sistema de posicionamiento en tiempo real para lugares cerrados (IPS)\\nutilizando los módulos DWM1000 bajo la tecnología Ultra Wide Band (UWB).\\nProgramado en C++ sobre un ATMEGA 328P. Resultados enviados mediante UDP a\\nservidor local por medio de módulo ESP8266 y presentados en python.\\nUWB; C++; Indoor Position System; WiFi; UDP; ESP8266l; Python;\\nDiseño y Layout de referencia Bandgap sin resistores de bajo consumo entre 0°C y\\n150°C en la tecnología XH018 — 2020\\nProyecto personal - Universidad de Buenos Aires\\nDiseño digital y layout de un circuito Bandgap Reference (B.G.R) sin resistores, con\\nuna tension central de 920 mV y una variación máxima de 20 mV entre 0°C y 150°C,\\nconsumo de potencia 3,6 uW.\\nSynopsys; HSpice; Python; Diseño de circuitos integrados; Microelectrónica\\nDiseño y Layout de circuito PLL digital en tecnología ONC5 — 2018\\nUniversidad de Buenos Aires - 86.46 Microelectrónica\\nDiseño digital y layout de un circuito Phase-Lock Loop (PLL) con una frecuencia\\ncentral de 72,5 MHz, pull-in range de 5.87 MHz y un lock range de 40MHz. Diseño\\nrealizado bajo la tecnología ONC5.\\nSynopsys; LTSpice; MATLAB; Diseño de circuitos integrados; Microelectrónica\\nDiseño y prototipado de un sistema de control de estabilidad — 2017\\nUniversidad de Buenos Aires - 86.06 Laboratorio de Microprocesadores\\nSistema de control de estabilidad programado en Assembler para ATMEL\\nAtmega328P o similar, utilizando módulo IMU MPU6050.\\nAssembler; Programación de microcontroladores; IMU; Sistemas de control\\nDiseño de un motor de rotación gráfico 3D basado en el algoritmo CORDIC\\nimplementado en VHDL para Nexys 2 — 2017\\nUniversidad de Buenos Aires - 86.41 Sistemas Digitales\\nDiseño de una arquitectura de rotación de objetos en 3 dimensiones por medio del\\nalgoritmo CORDIC (Coordinate Rotation Digital Computer), implementado en\\nlenguaje VHDL para FPGA Nexys Spartan 3E.\\nVHDL; FPGA;\\nIDIOMAS\\nInglés (C1)\\n- First Certificate of English (B2), Cambridge English\\n- Intensive English Program en University of Central Arkansas, Arkansas,\\nEstados Unidos.\\nFrancés (B2)\\n- IMT Atlantique curso B2 en Nantes, France.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b008f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNACIO TOMÁS DE PEDRO MERMIER INFORMACIÓN PERSONAL Nombre: Ignacio Tomás de Pedro Mermier Nacionalidad: Argentina Fecha de nacimiento: 14/01/1995 Edad: 29 años Correo electrónico: idepedro@fi.uba.ar EXPERIENCIA LABORAL 2023 - Actualidad — Allegro Microsystems — Analog Design Engineer Diseño de circuitos analógicos a nivel de bloque y top para circuitos integrados orientados a sensores magnéticos e inductivos. Verificación por medio de simulaciones por medio de herramientas específicas tales com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Reemplazar los saltos de línea con un espacio\n",
    "texto_cv_clean = texto_cv.replace('\\n', ' ')\n",
    "texto_cv_clean = texto_cv_clean.replace('\\n\\n', ' ')\n",
    "texto_cv_clean = re.sub(r'\\s+', ' ', texto_cv_clean).strip()\n",
    "print(texto_cv_clean[:500])  # Mostrar los primeros 500 caracteres del CV limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d63ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "nombre_indice = \"tp2-pln2\"\n",
    "PINECONE_ENVIRONMENT = \"us-west1-gcp\"\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a37f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# ================================\n",
    "\n",
    "def configurar_pinecone():\n",
    "    \"\"\"\n",
    "    Configura la conexión con Pinecone usando variables de entorno.\n",
    "    \n",
    "    Variables necesarias:\n",
    "    - PINECONE_API_KEY: Tu clave API de Pinecone\n",
    "    - PINECONE_ENVIRONMENT: El entorno de Pinecone (ej: 'us-west1-gcp')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener credenciales desde variables de entorno\n",
    "    api_key = PINECONE_API_KEY\n",
    "    environment = PINECONE_ENVIRONMENT\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"PINECONE_API_KEY no está configurada en las variables de entorno\")\n",
    "    \n",
    "    # Inicializar Pinecone\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    \n",
    "    print(f\"✅ Pinecone configurado correctamente en {environment}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f3b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneradorEmbeddings:\n",
    "    \"\"\"\n",
    "    Clase para generar embeddings usando diferentes modelos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, modelo: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Inicializa el generador de embeddings.\n",
    "        \n",
    "        Args:\n",
    "            modelo (str): Nombre del modelo de Sentence Transformers\n",
    "        \"\"\"\n",
    "        self.modelo_nombre = modelo\n",
    "        self.modelo = SentenceTransformer(modelo)\n",
    "        self.dimension = self.modelo.get_sentence_embedding_dimension()\n",
    "        \n",
    "        print(f\"✅ Modelo '{modelo}' cargado (dimensión: {self.dimension})\")\n",
    "    \n",
    "    def generar_embedding(self, texto: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Genera embedding para un texto individual.\n",
    "        \n",
    "        Args:\n",
    "            texto (str): Texto a convertir en embedding\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Vector de embedding\n",
    "        \"\"\"\n",
    "        embedding = self.modelo.encode(texto)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def generar_embeddings_lote(self, textos: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Genera embeddings para múltiples textos de manera eficiente.\n",
    "        \n",
    "        Args:\n",
    "            textos (List[str]): Lista de textos\n",
    "            \n",
    "        Returns:\n",
    "            List[List[float]]: Lista de vectores de embedding\n",
    "        \"\"\"\n",
    "        embeddings = self.modelo.encode(textos)\n",
    "        return [emb.tolist() for emb in embeddings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4934086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_indice(nombre_indice: str, dimension: int = 384, metrica: str = \"cosine\"):\n",
    "    \"\"\"\n",
    "    Crea un nuevo índice en Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice a crear\n",
    "        dimension (int): Dimensión de los vectores (depende del modelo de embedding)\n",
    "        metrica (str): Métrica de similitud ('cosine', 'euclidean', 'dotproduct')\n",
    "    \n",
    "    Configuración de infraestructura:\n",
    "        - Pods: Unidades de cómputo paralelo que procesan las consultas\n",
    "          • 1 pod = suficiente para desarrollo y proyectos pequeños\n",
    "          • Más pods = mayor capacidad de consultas simultáneas pero mayor costo\n",
    "        \n",
    "        - Réplicas: Copias idénticas del índice distribuidas geográficamente\n",
    "          • 1 réplica = configuración básica\n",
    "          • Más réplicas = mayor disponibilidad y tolerancia a fallos\n",
    "        \n",
    "        - Tipos de pod disponibles:\n",
    "          • p1.x1: 1 vCPU, ~5GB RAM (plan gratuito/starter)\n",
    "          • p1.x2: 2 vCPU, ~10GB RAM\n",
    "          • p1.x4: 4 vCPU, ~20GB RAM\n",
    "          • p2.x1: Optimizado para performance\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si se creó exitosamente\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Verificar si el índice ya existe\n",
    "    # indices_existentes = pinecone.list_indexes().names()\n",
    "    \n",
    "    # if nombre_indice in indices_existentes:\n",
    "    #     print(f\"⚠️  El índice '{nombre_indice}' ya existe\")\n",
    "    #     return True\n",
    "    \n",
    "\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "   \n",
    "        \n",
    "    # Crear el índice\n",
    "    if nombre_indice not in pc.list_indexes().names():\n",
    "        \n",
    "        pc.create_index(\n",
    "            name=nombre_indice,\n",
    "            dimension=dimension,\n",
    "            metric=metrica,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "            # pods=1,  # Pods: Unidades de cómputo que procesan queries. Más pods = mayor throughput pero mayor costo\n",
    "            # replicas=1,  # Réplicas: Copias del índice para alta disponibilidad. Más réplicas = mayor disponibilidad\n",
    "            # pod_type=\"p1.x1\"  # Tipo de pod: p1.x1 (gratuito, 1 vCPU), p1.x2 (2 vCPU), p2.x1 (optimizado), etc.\n",
    "        )\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        print(f\"ℹ️  El índice '{nombre_indice}' ya existe, no se creará uno nuevo\")\n",
    "        return True\n",
    "    \n",
    "    # Esperar a que el índice esté listo\n",
    "    print(f\"🔄 Creando índice '{nombre_indice}'...\")\n",
    "    # while nombre_indice not in pinecone.list_indexes():\n",
    "    #     time.sleep(1)\n",
    "    \n",
    "    print(f\"✅ Índice '{nombre_indice}' creado exitosamente\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5201de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 3. POBLACIÓN DEL ÍNDICE\n",
    "# ================================\n",
    "\n",
    "def poblar_indice_ejemplo(nombre_indice: str, generador: GeneradorEmbeddings):\n",
    "    \"\"\"\n",
    "    Puebla el índice con datos de ejemplo.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice de Pinecone\n",
    "        generador (GeneradorEmbeddings): Instancia del generador de embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Conectar al índice\n",
    "    indice = pc.Index(nombre_indice)\n",
    "    \n",
    "    # Generar los chunks del texto\n",
    "    print(\"🔄 Generando chunks del texto...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=360,chunk_overlap=20,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.create_documents([texto_cv_clean])\n",
    "    \n",
    "    print(f\"🔄 Poblando índice con {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Generar embeddings para todos los textos\n",
    "    textos = [chunk.page_content for chunk in chunks]\n",
    "    embeddings = generador.generar_embeddings_lote(textos)\n",
    "    \n",
    "    # Preparar datos para inserción en lotes\n",
    "    vectors_para_insertar = []\n",
    "    \n",
    "    for i, doc in enumerate(textos):\n",
    "        vector_data = {\n",
    "            \"id\": str(i),\n",
    "            \"values\": embeddings[i],\n",
    "            \"metadata\": {\n",
    "                \"texto\": str(doc),  # Aquí puedes agregar más metadata si es necesario\n",
    "                \"categoria\": \"cv\",  # Ejemplo de categoría\n",
    "            }\n",
    "        }\n",
    "        vectors_para_insertar.append(vector_data)\n",
    "    \n",
    "    # Insertar vectores en el índice\n",
    "    indice.upsert(vectors=vectors_para_insertar)\n",
    "    \n",
    "    # Verificar estadísticas del índice\n",
    "    estadisticas = indice.describe_index_stats()\n",
    "    print(f\"✅ Índice poblado exitosamente\")\n",
    "    print(f\"   📊 Total de vectores: {estadisticas['total_vector_count']}\")\n",
    "    print(f\"   📏 Dimensión: {estadisticas['dimension']}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 4. BÚSQUEDAS EN EL ÍNDICE\n",
    "# ================================\n",
    "\n",
    "def buscar_documentos_similares(\n",
    "    nombre_indice: str, \n",
    "    consulta: str, \n",
    "    generador: GeneradorEmbeddings,\n",
    "    top_k: int = 3,\n",
    "    filtro_metadata: Dict = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda por similitud en el índice.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice de Pinecone\n",
    "        consulta (str): Texto de consulta para buscar\n",
    "        generador (GeneradorEmbeddings): Generador de embeddings\n",
    "        top_k (int): Número de resultados más similares a devolver\n",
    "        filtro_metadata (Dict): Filtros opcionales por metadata\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Lista de documentos similares con scores\n",
    "    \"\"\"\n",
    "    \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Conectar al índice\n",
    "    indice = pc.Index(nombre_indice)\n",
    "    \n",
    "    # Generar embedding para la consulta\n",
    "    print(f\"🔍 Buscando documentos similares a: '{consulta}'\")\n",
    "    embedding_consulta = generador.generar_embedding(consulta)\n",
    "    \n",
    "    # Realizar la búsqueda\n",
    "    resultados = indice.query(\n",
    "        vector=embedding_consulta,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=filtro_metadata\n",
    "    )\n",
    "    \n",
    "    # Procesar y formatear resultados\n",
    "    documentos_encontrados = []\n",
    "    \n",
    "    print(f\"\\n📋 Resultados encontrados ({len(resultados['matches'])}):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, match in enumerate(resultados['matches'], 1):\n",
    "        documento = {\n",
    "            \"posicion\": i,\n",
    "            \"id\": match[\"id\"],\n",
    "            \"score\": round(match[\"score\"], 4),\n",
    "            \"texto\": match[\"metadata\"][\"texto\"],\n",
    "        }\n",
    "        \n",
    "        documentos_encontrados.append(documento)\n",
    "        \n",
    "        # Mostrar resultado formateado\n",
    "        print(f\"{i}. ID: {documento['id']}\")\n",
    "        print(f\"   📊 Score: {documento['score']}\")\n",
    "        print(f\"   📝 Texto: {documento['texto'][:100]}...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return documentos_encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdbe39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 5. GESTIÓN DEL ÍNDICE\n",
    "# ================================\n",
    "\n",
    "def obtener_estadisticas_indice(nombre_indice: str):\n",
    "    \"\"\"\n",
    "    Muestra estadísticas detalladas del índice.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice\n",
    "    \"\"\"\n",
    "    \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Conectar al índice\n",
    "    indice = pc.Index(nombre_indice)\n",
    "    estadisticas = indice.describe_index_stats()\n",
    "    \n",
    "    print(f\"\\n📊 ESTADÍSTICAS DEL ÍNDICE '{nombre_indice}'\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📦 Total de vectores: {estadisticas.get('total_vector_count', 0)}\")\n",
    "    print(f\"📏 Dimensión: {estadisticas.get('dimension', 0)}\")\n",
    "    \n",
    "    # Mostrar estadísticas por namespace si existen\n",
    "    if 'namespaces' in estadisticas:\n",
    "        print(f\"🏷️  Namespaces:\")\n",
    "        for namespace, stats in estadisticas['namespaces'].items():\n",
    "            print(f\"   - {namespace}: {stats.get('vector_count', 0)} vectores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36f9c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_con_filtros_ejemplo(nombre_indice: str, generador: GeneradorEmbeddings):\n",
    "    \"\"\"\n",
    "    Demuestra búsquedas con filtros de metadata.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice\n",
    "        generador (GeneradorEmbeddings): Generador de embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 EJEMPLO DE BÚSQUEDAS CON FILTROS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Búsqueda 1: Sin filtros\n",
    "    print(\"\\n1️⃣:\")\n",
    "    buscar_documentos_similares(\n",
    "        nombre_indice, \n",
    "        \"machine learning\", \n",
    "        generador,\n",
    "        top_k=3\n",
    "    )\n",
    "    \n",
    "    # Búsqueda 2: Con filtro por categoría\n",
    "    print(\"\\n2️⃣:\")\n",
    "    buscar_documentos_similares(\n",
    "        nombre_indice,\n",
    "        \"processamiento de datos\",\n",
    "        generador,\n",
    "        top_k=2,\n",
    "    )\n",
    "    \n",
    "    # Búsqueda 3: Con filtro por fecha\n",
    "    print(\"\\n3️⃣:\")\n",
    "    buscar_documentos_similares(\n",
    "        nombre_indice,\n",
    "        \"redes neuronales\",\n",
    "        generador,\n",
    "        top_k=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef80b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_indice_completo(nombre_indice: str):\n",
    "    \"\"\"\n",
    "    Elimina todos los vectores del índice.\n",
    "    \n",
    "    Args:\n",
    "        nombre_indice (str): Nombre del índice a limpiar\n",
    "    \"\"\"\n",
    "    \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Conectar al índice\n",
    "    indice = pc.Index(nombre_indice)\n",
    "    \n",
    "    print(f\"🧹 Limpiando índice '{nombre_indice}' completamente...\")\n",
    "    indice.delete(delete_all=True)\n",
    "    \n",
    "    print(\"✅ Índice limpiado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b6f463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Limpiando índice 'tp2-pln2' completamente...\n",
      "✅ Índice limpiado exitosamente\n"
     ]
    }
   ],
   "source": [
    "limpiar_indice_completo(nombre_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4831c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIANDO EJEMPLO DE PINECONE\n",
      "==================================================\n",
      "✅ Pinecone configurado correctamente en us-west1-gcp\n",
      "✅ Modelo 'sentence-transformers/all-MiniLM-L6-v2' cargado (dimensión: 384)\n",
      "ℹ️  El índice 'tp2-pln2' ya existe, no se creará uno nuevo\n",
      "🔄 Generando chunks del texto...\n",
      "🔄 Poblando índice con 22 chunks...\n",
      "✅ Índice poblado exitosamente\n",
      "   📊 Total de vectores: 22\n",
      "   📏 Dimensión: 384\n",
      "\n",
      "📊 ESTADÍSTICAS DEL ÍNDICE 'tp2-pln2'\n",
      "==================================================\n",
      "📦 Total de vectores: 22\n",
      "📏 Dimensión: 384\n",
      "🏷️  Namespaces:\n",
      "   - : 22 vectores\n",
      "\n",
      "🔍 EJEMPLO DE BÚSQUEDAS CON FILTROS\n",
      "==================================================\n",
      "\n",
      "1️⃣:\n",
      "🔍 Buscando documentos similares a: 'machine learning'\n",
      "\n",
      "📋 Resultados encontrados (3):\n",
      "================================================================================\n",
      "1. ID: 11\n",
      "   📊 Score: 0.3452\n",
      "   📝 Texto: (EAMTA) - Machine Learning en VLSI - Presentador: Dr. Ing. Pedro Julian - Track básico VLSI - 2017 -...\n",
      "--------------------------------------------------------------------------------\n",
      "2. ID: 5\n",
      "   📊 Score: 0.2205\n",
      "   📝 Texto: INASE, incluyendo manejo de FrontEnd, BackEnd y administración de la Base de Datos, utilizando Djang...\n",
      "--------------------------------------------------------------------------------\n",
      "3. ID: 14\n",
      "   📊 Score: 0.1228\n",
      "   📝 Texto: el desarrollo de la tesis de grado dentro de los laboratorios de Microelectrónica (uELAB) y Laborato...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2️⃣:\n",
      "🔍 Buscando documentos similares a: 'processamiento de datos'\n",
      "\n",
      "📋 Resultados encontrados (2):\n",
      "================================================================================\n",
      "1. ID: 4\n",
      "   📊 Score: 0.2999\n",
      "   📝 Texto: cámaras IP con el objetivo de filmar y transmitir en vivo, utilizando el protocolo RTSP hacia un ser...\n",
      "--------------------------------------------------------------------------------\n",
      "2. ID: 14\n",
      "   📊 Score: 0.2778\n",
      "   📝 Texto: el desarrollo de la tesis de grado dentro de los laboratorios de Microelectrónica (uELAB) y Laborato...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3️⃣:\n",
      "🔍 Buscando documentos similares a: 'redes neuronales'\n",
      "\n",
      "📋 Resultados encontrados (5):\n",
      "================================================================================\n",
      "1. ID: 21\n",
      "   📊 Score: 0.1444\n",
      "   📝 Texto: of English (B2), Cambridge English - Intensive English Program en University of Central Arkansas, Ar...\n",
      "--------------------------------------------------------------------------------\n",
      "2. ID: 14\n",
      "   📊 Score: 0.1118\n",
      "   📝 Texto: el desarrollo de la tesis de grado dentro de los laboratorios de Microelectrónica (uELAB) y Laborato...\n",
      "--------------------------------------------------------------------------------\n",
      "3. ID: 9\n",
      "   📊 Score: 0.098\n",
      "   📝 Texto: Facultad de Ingeniería Universidad de Buenos Aires — Marzo 2019 - Diciembre 2019 Título: “Estudio de...\n",
      "--------------------------------------------------------------------------------\n",
      "4. ID: 20\n",
      "   📊 Score: 0.0965\n",
      "   📝 Texto: implementado en VHDL para Nexys 2 — 2017 Universidad de Buenos Aires - 86.41 Sistemas Digitales Dise...\n",
      "--------------------------------------------------------------------------------\n",
      "5. ID: 11\n",
      "   📊 Score: 0.075\n",
      "   📝 Texto: (EAMTA) - Machine Learning en VLSI - Presentador: Dr. Ing. Pedro Julian - Track básico VLSI - 2017 -...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 BÚSQUEDA PERSONALIZADA\n",
      "==============================\n",
      "🔍 Buscando documentos similares a: 'Nacionalidad'\n",
      "\n",
      "📋 Resultados encontrados (2):\n",
      "================================================================================\n",
      "1. ID: 0\n",
      "   📊 Score: 0.2758\n",
      "   📝 Texto: IGNACIO TOMÁS DE PEDRO MERMIER INFORMACIÓN PERSONAL Nombre: Ignacio Tomás de Pedro Mermier Nacionali...\n",
      "--------------------------------------------------------------------------------\n",
      "2. ID: 10\n",
      "   📊 Score: 0.246\n",
      "   📝 Texto: Proyecto UBACyT — 2019 - 2020 Título: \"Nuevas estructuras y técnicas de simulación y control para co...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ EJEMPLO COMPLETADO EXITOSAMENTE\n",
      "📁 Índice 'tp2-pln2' está listo para usar\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ejecuta un ejemplo completo de uso de Pinecone:\n",
    "1. Configuración\n",
    "2. Creación del índice\n",
    "3. Población con datos\n",
    "4. Búsquedas de ejemplo\n",
    "5. Limpieza (opcional)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"🚀 INICIANDO EJEMPLO DE PINECONE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Configurar conexión\n",
    "    configurar_pinecone()\n",
    "    \n",
    "    # 2. Inicializar generador de embeddings\n",
    "    generador = GeneradorEmbeddings()\n",
    "    \n",
    "    # 3. Crear índice\n",
    "    crear_indice(nombre_indice, dimension=generador.dimension)\n",
    "    \n",
    "    # 4. Poblar índice con datos de ejemplo\n",
    "    poblar_indice_ejemplo(nombre_indice, generador)\n",
    "    \n",
    "    # 5. Mostrar estadísticas\n",
    "    obtener_estadisticas_indice(nombre_indice)\n",
    "    \n",
    "    # 6. Realizar búsquedas de ejemplo\n",
    "    buscar_con_filtros_ejemplo(nombre_indice, generador)\n",
    "    \n",
    "    # 7. Búsqueda personalizada\n",
    "    print(\"\\n🎯 BÚSQUEDA PERSONALIZADA\")\n",
    "    print(\"=\" * 30)\n",
    "    consulta_personalizada = \"Nacionalidad\"\n",
    "    resultados = buscar_documentos_similares(\n",
    "        nombre_indice, \n",
    "        consulta_personalizada, \n",
    "        generador,\n",
    "        top_k=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ EJEMPLO COMPLETADO EXITOSAMENTE\")\n",
    "    print(f\"📁 Índice '{nombre_indice}' está listo para usar\")\n",
    "    \n",
    "    # Opcional: Comentar la siguiente línea si quieres mantener el índice\n",
    "    # eliminar_indice(nombre_indice)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error durante la ejecución: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa613f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113854/50843343.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore  \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "# Conectar al índice\n",
    "indice = pc.Index(nombre_indice)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_field = \"texto\"  \n",
    "vectorstore = PineconeVectorStore(  \n",
    "    indice, embeddings, text_field  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb20fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='25', metadata={'categoria': 'cv'}, page_content='(EAMTA) - Machine Learning en VLSI - Presentador: Dr. Ing. Pedro Julian - Track básico VLSI - 2017 - Presentador: Dr. Ing. Pedro Julián - Diseño de circuitos integrados para aplicaciones implacables'),\n",
       " Document(id='24', metadata={'categoria': 'cv'}, page_content='Laboratorio de Control de Accionamientos, Tracción y Potencia FORMACIÓN COMPLEMENTARIA Escuela Argentina de Micro y Nano Tecnología y Aplicaciones (EAMTA) - Machine Learning en VLSI - Presentador:'),\n",
       " Document(id='16', metadata={'categoria': 'cv'}, page_content='Ingeniería Universidad de Buenos Aires — 2024 - Actualidad Título: Master en Inteligencia Artificial Embebida. Facultad de Ingeniería Universidad de Buenos Aires — 2014 - 2019 Título: Ingeniero')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Machine Learning y Deep Learning\"  \n",
    "vectorstore.similarity_search(  \n",
    "    query,  # our search query  \n",
    "    k=3  # return 3 most relevant docs  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ee4dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq           # Cliente oficial de Groq para acceso a LLMs\n",
    "from langchain_groq import ChatGroq              # Integración LangChain-Groq\n",
    "\n",
    "model = \"llama3-8b-8192\"  # Modelo Groq seleccionado por el usuario\n",
    "groq_chat = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,     # Clave API para autenticación\n",
    "    model_name=model,              # Modelo seleccionado por el usuario\n",
    "    temperature=0.7,               # Creatividad de las respuestas (0=determinista, 1=creativo)\n",
    "    max_tokens=1000,               # Máximo número de tokens en la respuesta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7ab713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según la información proporcionada, Ignacio Tomás de Pedro Mermier tiene la nacionalidad Argentina.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA  \n",
    "\n",
    "query = \"Todas las nacionalidades de Ignacio\"\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=groq_chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "# qa_cv = RetrievalQA.from_chain_type(  \n",
    "#     llm=groq_chat,  \n",
    "#     chain_type=\"stuff\",  \n",
    "#     retriever=vectorstore_cv.as_retriever()  \n",
    "# ) \n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
